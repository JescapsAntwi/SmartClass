import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    LlamaTokenizerFast, # Llama 3 uses LlamaTokenizerFast
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Configuration ---
MODEL_NAME = "meta-llama/Llama-3.2-1B"
FINETUNE_DATA_FILE = "./syllabus_finetune_data.jsonl"  # Generated by prepare_data_for_finetune.py
OUTPUT_DIR = "./llama3.2-1b-syllabus-finetuned" # Directory to save the fine-tuned model adapter

# LoRA configuration
LORA_R = 16                # Rank of the LoRA matrices
LORA_ALPHA = 32            # Alpha parameter for LoRA scaling
LORA_DROPOUT = 0.05        # Dropout probability for LoRA layers
# Specify target modules for LoRA. Common for Llama: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
LORA_TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Training arguments
NUM_TRAIN_EPOCHS = 1 # Start with 1-3 epochs and evaluate
PER_DEVICE_TRAIN_BATCH_SIZE = 2 # Adjust based on your GPU VRAM (1, 2, or 4 are common for 1B models)
GRADIENT_ACCUMULATION_STEPS = 2 # Effective batch size = batch_size * accumulation_steps
LEARNING_RATE = 2e-4
WEIGHT_DECAY = 0.001
OPTIM_PAGED_ADAMW = "paged_adamw_8bit" # Use paged AdamW for memory efficiency if using 8-bit
MAX_GRAD_NORM = 0.3
WARMUP_RATIO = 0.03
LR_SCHEDULER_TYPE = "cosine"
MAX_SEQ_LENGTH = 1024 # Adjust based on your data and VRAM. Llama 3.2 1B context length is 131072, but fine-tuning often uses smaller.

# Quantization config (for QLoRA - 4-bit NormalFloat)
USE_4BIT_QUANTIZATION = True # Set to True to use QLoRA, False for LoRA without 4-bit.
BNB_4BIT_COMPUTE_DTYPE = "bfloat16" # Or "float16" if bfloat16 not supported
BNB_4BIT_QUANT_TYPE = "nf4" # "nf4" (NormalFloat4) or "fp4"

def main():
    logging.info(f"Starting fine-tuning for model: {MODEL_NAME}")

    # 1. Load Tokenizer
    # Llama 3 uses LlamaTokenizerFast. Ensure you have sentencepiece installed.
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    
    # Llama models usually don't have a pad token. Add one if it's missing.
    # Using EOS token as PAD token is a common practice for Llama.
    if tokenizer.pad_token is None:
        logging.info("Tokenizer does not have a pad_token. Setting pad_token to eos_token.")
        tokenizer.pad_token = tokenizer.eos_token
        # Important: The model's config should also reflect this if you are not resizing embeddings.
        # For SFTTrainer, it often handles padding internally based on tokenizer.pad_token.

    logging.info(f"Tokenizer loaded. Pad token ID: {tokenizer.pad_token_id}")

    # 2. Load Dataset
    logging.info(f"Loading dataset from {FINETUNE_DATA_FILE}")
    dataset = load_dataset("json", data_files=FINETUNE_DATA_FILE, split="train")
    logging.info(f"Dataset loaded. Number of examples: {len(dataset)}")
    # You might want to shuffle the dataset: dataset = dataset.shuffle(seed=42)

    # 3. Configure BitsAndBytes for 4-bit quantization (QLoRA)
    bnb_config = None
    if USE_4BIT_QUANTIZATION:
        compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=BNB_4BIT_QUANT_TYPE,
            bnb_4bit_compute_dtype=compute_dtype,
            bnb_4bit_use_double_quant=False, # Optional
        )
        logging.info("Using 4-bit quantization (QLoRA).")

    # 4. Load Pre-trained Model
    logging.info(f"Loading base model: {MODEL_NAME}")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config if USE_4BIT_QUANTIZATION else None,
        device_map="auto", # Automatically distributes model across available GPUs/CPU
        trust_remote_code=True,
        torch_dtype=compute_dtype if USE_4BIT_QUANTIZATION and bnb_config else torch.bfloat16 # Use bfloat16 for faster training if not quantizing to 4-bit or if compute_dtype is bfloat16
    )
    # If not using device_map="auto" and loading on a single GPU: model.to(device)
    
    # Resize token embeddings if a new pad token was added *and* it's a new token, not an alias.
    # Since we set pad_token = eos_token, resizing is usually not strictly necessary here.
    # model.config.pad_token_id = tokenizer.pad_token_id # Ensure model config knows about pad token

    # Prepare model for k-bit training if using quantization
    if USE_4BIT_QUANTIZATION:
        model = prepare_model_for_kbit_training(model)
        logging.info("Model prepared for k-bit training.")

    # 5. Configure LoRA
    peft_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        target_modules=LORA_TARGET_MODULES,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, peft_config)
    logging.info("LoRA configured and PEFT model created.")
    model.print_trainable_parameters()

    # 6. Set up Training Arguments
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_TRAIN_EPOCHS,
        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        optim=OPTIM_PAGED_ADAMW if USE_4BIT_QUANTIZATION else "adamw_torch",
        fp16=False, # Set to True if not using bfloat16 and your GPU supports fp16
        bf16=True if not USE_4BIT_QUANTIZATION and torch.cuda.is_bf16_supported() else False, # Enable bf16 if supported and not using 4-bit
        max_grad_norm=MAX_GRAD_NORM,
        warmup_ratio=WARMUP_RATIO,
        lr_scheduler_type=LR_SCHEDULER_TYPE,
        logging_steps=25, # Log every 25 steps
        save_strategy="epoch", # Save checkpoints at the end of each epoch
        report_to="tensorboard", # Or "wandb", "none"
        # ddp_find_unused_parameters=False, # Set to False if using DDP and encountering issues
    )

    # 7. Initialize SFTTrainer
    trainer = SFTTrainer(
        model=model,
        train_dataset=dataset,
        peft_config=peft_config, # Pass peft_config again if not already applied, but get_peft_model does this
        dataset_text_field="text", # Name of the column in your dataset that contains the formatted text
        max_seq_length=MAX_SEQ_LENGTH,
        tokenizer=tokenizer,
        args=training_args,
        packing=False, # Set to True if your dataset is pre-packed for efficiency, False otherwise
    )

    # 8. Start Fine-tuning
    logging.info("Starting training...")
    trainer.train()
    logging.info("Training finished.")

    # 9. Save the Fine-tuned Model Adapter
    logging.info(f"Saving LoRA adapter to {OUTPUT_DIR}")
    trainer.model.save_pretrained(OUTPUT_DIR) # Saves only the LoRA adapter
    tokenizer.save_pretrained(OUTPUT_DIR) # Save tokenizer for easy loading later
    logging.info("Model adapter and tokenizer saved.")

    # To save the full model (merged):
    merged_model = model.merge_and_unload()
    merged_model.save_pretrained(os.path.join(OUTPUT_DIR, "final_merged_checkpoint"))
    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, "final_merged_checkpoint"))
    logging.info("Full merged model saved (optional).")

if __name__ == "__main__":
    main()